{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Data Scientist Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Past Experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From highest business impact to lowest:\n",
    "1. quality score\n",
    "    * automate repetitive tasks, data-driven pricing\n",
    "    * time series as cross-section data; multiple features, missing data, avoid assumption of stationarity for short time periods\n",
    "    * feature engineering; how many days needed helps business makes better contracts\n",
    "    * smaller models with good features are easier to productionize\n",
    "2. crm customer lookalike\n",
    "    * offline to online conversion\n",
    "    * very imbalanced dataset; supervised learning is useless\n",
    "    * rule-based, PCA, autoencoders\n",
    "3. search\n",
    "    * show things people will most likely buy\n",
    "    * full-text match\n",
    "    * learning-to-rank with product embeddings\n",
    "4. recommendation\n",
    "    * personalized with collaborative filtering\n",
    "    * association rules with collaborative filtering\n",
    "    * similar products with category classifier\n",
    "5. abnormality detection in xrays\n",
    "    * pretrained models with 3 channels to 1 channel\n",
    "    * normalization of images\n",
    "    * false discovery rate at 93% recall\n",
    "6. text classification in thai\n",
    "    * ULMFit over BERT with cleaning rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a stationary time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stationary process has the property that \n",
    "* the mean, \n",
    "* variance and \n",
    "* autocorrelation structure \n",
    "do not change over time\n",
    "\n",
    "Dickey-Fuller test tests if coefficient $\\rho-1$ is zero\n",
    "\n",
    "$$y_t - y_{t-1} = (\\rho - 1) y_{t-1} + e_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA in [R](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/) and [Python](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `AR` - autoregressive\n",
    "\n",
    "$$y_t = \\beta_0 y_{t-1} + e_t$$\n",
    "\n",
    "2. `I` - differenceing\n",
    "\n",
    "$$z_t = y_t - y_{t-1}$$\n",
    "\n",
    "3. `MA` - moving average (of errors)\n",
    "\n",
    "$$y_t = \\beta_0 e_{t-1} + e_t$$\n",
    "\n",
    "Variations\n",
    "* `ARIMA(1,0,0)` = first-order autoregressive model\n",
    "* `ARIMA(0,1,0)` = random walk\n",
    "* `ARIMA(1,1,0)` = differenced first-order autoregressive model\n",
    "* `ARIMA(0,1,1)` without constant = simple exponential smoothing\n",
    "* `ARIMA(0,1,1)` with constant = simple exponential smoothing with growth\n",
    "* `ARIMA(0,2,1)` or (0,2,2) without constant = linear exponential smoothing\n",
    "* `ARIMA(1,1,2)` with constant = damped-trend linear exponential smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bernoulli - probability of boolean outcomes with probability $p$\n",
    "* Binomial - sum of results of $n$ Bernoulli trials with probability $p$\n",
    "* Hypergeometric - Binomial but without replacements\n",
    "* Poisson - continuous version of binomial with rate of events per time interval $\\lambda$\n",
    "* Exponential - time between Poisson events with rate $\\lambda$\n",
    "* Weibull - generalized Exponential with varying rates $\\lambda_i$\n",
    "* Geometric - how many failures before a success with probability $p$\n",
    "* Negative binomial - how many failures before $n$ successes with probability $p$\n",
    "* Normal distribution - sample means of anything independent and identically distributed\n",
    "* Standard normal distribution - N(0,1)\n",
    "* t distribution - fatter-tailed normal distribution\n",
    "* Chi-square distribution - sum of squares of normally distributed random variables\n",
    "* F distribution - ratio of two chi-squared random variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the differences between PMF, PDF and CDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `PMF` - actual probability of discrete value of a random variable\n",
    "* `PDF` - sum of probability of a continuous random variable at a given range of values\n",
    "* `CDF` - sum of probability of a continuous random variable up to a give nvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some commonly used performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification\n",
    "* precision - $\\frac{tp}{tp+fp}$; positive predicted value\n",
    "* negative precision - $\\frac{tn}{tn+fn}$; negative predicted value\n",
    "* recall - $\\frac{tp}{tp+fn}$; true positive rate, sensitivity, power\n",
    "* negative recall - $\\frac{tn}{tn+fp}$; true negative rate, specificity\n",
    "* false positive rate - $\\frac{fp}{fp+tn}$; type 1 error\n",
    "* false negative rate - $\\frac{fn}{fn+tp}$; type 2 error\n",
    "* false discovery rate - $\\frac{fp}{tp+fp}$; 1-precision\n",
    "* false omission rate - $\\frac{fn}{tn+fn}$; 1-negative precision\n",
    "* F-score - $F_\\beta = \\frac {(1 + \\beta^2) \\cdot \\mathrm{true\\ positive} }{(1 + \\beta^2) \\cdot \\mathrm{true\\ positive} + \\beta^2 \\cdot \\mathrm{false\\ negative} + \\mathrm{false\\ positive}}$\n",
    "* Area under ROC curve - FPR vs TPR\n",
    "* kappa - $\\kappa \\equiv {\\frac {p_{o}-p_{e}}{1-p_{e}}}=1-{\\frac {1-p_{o}}{1-p_{e}}}$; $p_o$ is observed agreement, $p_e$ is expected agreement\n",
    "* [BLEU (bilingual evaluation understudy)](https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4) - exponentially weighted precision of 1-4 grams with brevity penalty\n",
    "\n",
    "regression\n",
    "* MSE - $\\frac{1}{n} \\sum_{i=1}^{n} (y_i-\\hat{y_i})^2$\n",
    "* MAE - $\\frac{1}{n} \\sum_{i=1}^{n} |y_i-\\hat{y_i}|$\n",
    "* $\\text{variation in y}$ - $\\frac{1}{n} \\sum_{i=1}^{n} (y_i-\\bar{y_i})^2$\n",
    "* $R^2$ - $\\frac{MSE}{\\text{variation in y}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What hypothesis tests to use for which occasions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ratio vs ratio - proportional Z-test\n",
    "* real number vs real number - t-test\n",
    "* categorical vs categorical - Chi-squared test\n",
    "* categorical vs real number - (M)ANOVA; F-distribution\n",
    "* [lady tasting tea](https://en.wikipedia.org/wiki/Lady_tasting_tea)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(X) = \\Sigma_{x} p(x) * log_2 p(x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99, 0.01] 0.08079313589591118\n",
      "[0.9, 0.1] 0.4689955935892812\n",
      "[0.5, 0.5] 1.0\n",
      "[0.1, 0.9] 0.4689955935892812\n",
      "[0.01, 0.99] 0.08079313589591118\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(X):\n",
    "    res = 0\n",
    "    for x in X: res+=x*np.log2(x)\n",
    "    return -res\n",
    "\n",
    "Xs = [[0.99,0.01],[0.9,0.1],[0.5,0.5],[0.1,0.9],[0.01,0.99]]\n",
    "for X in Xs: print(X,entropy(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some common loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinge loss\n",
    "$$L_{i} = \\Sigma max(0,f_{j} - f_{target} + \\Delta)$$\n",
    "where $j!=target$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#3 rows, 5 classes 0-4\n",
    "y = torch.tensor([1, 0, 4])\n",
    "logit = torch.randn(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "loss(x,class) &= −x_{class} \\\\\n",
    "L &= \\Sigma loss(x,class)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1049), tensor(-0.1049))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#negative log likelihood in pytorch is just negative aggregation (NO LOG!)\n",
    "F.nll_loss(logit,y), -torch.gather(logit,1,y[:,None]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "loss(x,class) &=−log(\\frac{e^{x_{class}}}{\\Sigma e^{x_j}}) \\\\\n",
    "L &= \\Sigma loss(x,class)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.3862), tensor(1.3862))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross entropy in pytorch = log_softmax + nll\n",
    "F.cross_entropy(logit,y), -torch.gather(F.log_softmax(logit,1),1,y[:,None]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [0.],\n",
       "         [1.]]),\n",
       " tensor([[-0.0066],\n",
       "         [-1.0632],\n",
       "         [-1.2841]]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#binary cross entropy\n",
    "y = torch.randint(0,2,(3,1)).float()\n",
    "logit = torch.randn(3, 1)\n",
    "y,logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8405), tensor(0.8405))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#binary_cross_entropy_with_logits in pytorch = sigmoid + binary cross entropy\n",
    "F.binary_cross_entropy_with_logits(logit,y),F.binary_cross_entropy(torch.sigmoid(logit),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.2401), tensor(2.2401))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mse\n",
    "y = torch.randn(3, 1)\n",
    "y_hat = torch.randn(3, 1)\n",
    "F.mse_loss(y_hat,y), (y-y_hat).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9315), tensor(0.9315))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mae\n",
    "y = torch.randn(3, 1)\n",
    "y_hat = torch.randn(3, 1)\n",
    "F.l1_loss(y_hat,y), (y-y_hat).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some common layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Softmax - $\\frac{exp(x_i)}{\\Sigma exp(x_j)}$; range 0-1\n",
    "* Sigmoid - $\\frac{1}{1+ exp(-x)}$; range 0-1\n",
    "* Tanh - range -1 to 1\n",
    "* ReLU \n",
    "* BatchNorm\n",
    "* Dropout\n",
    "* ConvXD\n",
    "* RNN, LSTM, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some common distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cosineSimilarity = \\frac{A \\cdot B}{||A||_{2}||B||_{2}} = \\frac{\\Sigma A_i B_i}{\\sqrt{\\Sigma A_i^2 \\Sigma B_i^2}}$$\n",
    "\n",
    "$$JaccardSimilarity = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|}$$\n",
    "\n",
    "$$Manhattan(l1) = \\Sigma|A_i-B_i|$$\n",
    "\n",
    "$$Euclidean(l2) =\\sqrt{\\Sigma(A_i-B_i)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is bias-variance tradeoff\n",
    "\n",
    "Bias is the error of prediction and variance is how spread out the prediction is. High bias is underfitting and high variance is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
